---
title: "The 'Hello World' of Deep Learning"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Welcome to your first deep learning module!

This module is designed to provide you an introduction to deep learning and 
some of the key components that make DL algorithms run. Throughout this case 
study you will learn:

* What tensors are
* What makes up a basic feedforward neural network architecture
* What forward and backward passes are
* How a neural network learns via backpropagation and batch processing

In doing so you will run your first neural network with one of the most famous 
benchmark data sets --> MNIST.

# Package Requirements

Normally, you will be starting out from scratch and need to install and set up
keras on your own laptop. First, you need to install keras from CRAN. Once the
package is installed, you need to install the Keras and TensorFlow Python
packages, which is what the R Keras and TensorFlow packages communicate
with. keras simplifies this with `install_keras()` which allows for:

* both GPU & CPU options setups
* installation in a virtual or conda environment
* setup for Theano & CNTK backends rather than TensorFlow

```{r install, eval = FALSE}
# install keras if necessary
# install.packages("keras")

# default CPU-based installations of Keras and TensorFlow
# install_keras()

# for GPU installation
# install_keras(tensorflow = "gpu")
```

This installation can be quite simple, and it can get quite complex depending on
your current system setup and needs. 

For this workshop we will be using a cloud environment to ensure we are all 
operating in common environment and Keras and TensorFlow have already been 
installed.

Let's load the `keras` package along with a couple other packages we'll use.

```{r, message=FALSE, warning=FALSE}
library(keras)     # for deep learning
library(dplyr)     # for minor data wrangling
library(ggplot2)   # for plotting
```


# Part 1: Data Preparation

## Obtain data

`keras` has many built in data sets (or functions to automatically install data
sets). Check out what data is available with `dataset_` + tab.

We're going to use the __MNIST__ data set which is the "hello world" for 
learning deep learning! [ℹ️](http://bit.ly/dl-01#2)

```{r data}
mnist <- dataset_mnist()
str(mnist)
```

When we work with keras:

* training and test sets need to be independent
* features and labels (aka target, response) need to be independent
* use `%<-%` for ___object unpacking___ (see `?zeallot::%<-%` or [ℹ️](http://bit.ly/dl-01#5))

```{r extract-train-test}
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
```

## Data structure

Our training images (aka features) are stored as a 3D array

* 60,000 images consisting of a...
* 28x28 matrix with...
* values ranging from 0-255 representing gray scale pixel values.

```{r features}
dim(train_images)
```

Check out the first digit

```{r first-digit}
digit <- train_images[1,,]
digit
```

Lets plot the first digit and compare to the above matrix

```{r plot-first-digit}
plot(as.raster(digit, max = 255))
```

Now lets check out the first 100 digits

```{r plot-first-100-digits}
par(mfrow = c(10, 10), mar = c(0,0,0,0))
for (i in 1:100) {
  plot(as.raster(train_images[i,,], max = 255))
}
```

## Reshape data:

Our current data structure is incompatible for modeling the basic neural network
that we are going to start with.

```{r current-structure-features}
str(train_images)
str(test_images)
```

We need to reshape these into a 2D ___tensor___ [ℹ️](http://bit.ly/dl-01#11).
Similar to how we can reshape a single matrix into a vector:

```{r}
m <- matrix(1:9, ncol = 3)
m

as.vector(m)
```

We can reshape a 3D array to a 2D array:

```{r reshape-to-2D-tensor}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
test_images <- array_reshape(test_images, c(10000, 28 * 28))

str(train_images)
str(test_images)
```


## Normalize data:

Our feature values range from 0-255. For our purposes we want to normalize these
values to be between 0-1. We'll discuss why this is important later.

```{r normalize-feature-values}
train_images <- train_images / 255
test_images <- test_images / 255
```


## Prepare labels:

Our response data are currently 1D arrays (60,000 vectors of length 1) with the
actual digit the image represents.

```{r current-structure-labels}
str(train_labels)
str(test_labels)
```

We could use that structure for our response but, often, for classification 
problems we'll reformat with `to_categorical()`:

```{r reshape-labels}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)

head(train_labels)
```


# Part 2: Training a DL model

Nearly all DL models have a very similar training process:

1. Define the network architecture
2. Define network compilation
3. Execute training loop

Let's run the next 4 code chunks first, then we'll come back and explain what
each line of code is doing.

## 1. Define the network architecture [ℹ️](http://bit.ly/dl-01#24)

```{r architecture}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = 'relu', input_shape = ncol(train_images)) %>%
  layer_dense(units = 10, activation = 'softmax')
```

You can view a summary of the network

```{r summary}
summary(network)
```

## 2. Define network compilation [ℹ️](http://bit.ly/dl-01#49)

```{r compile}
network %>% compile(
  loss = "categorical_crossentropy",
  optimizer = "rmsprop",
  metrics = c("accuracy")
)
```

## 3. Execute training loop [ℹ️](http://bit.ly/dl-01#79)

```{r train}
# Each epoch is ~ 4sec, total time <2 min
history <- network %>% 
  fit(train_images, train_labels, 
      batch_size = 128, epochs = 20, 
      validation_split = 0.2)
```

When training the model in RStudio, you will see a real-time plot of the loss
metric along with any other metrics identified to track.

You can also `plot` this trained model object, which just uses `ggplot2` and 
can be modified like any other `ggplot2` plot (as long as `ggplot2` is loaded.

```{r plotTraining}
plot(history) + ggtitle("My first deep learning model!")
```


# YOUR TURN! 

1. Poke around at the `history` object, what information does it contain?
2. Try re-running the model with more or less hidden units or add another hidden
   layer. How do your results change?
3. Pick a final model structure and using the optimal number of epochs for your
   model that minimizes the validation loss (`history$metrics$val_loss`)?
4. Fill in the blanks below and re-run the model with this number of epochs.

```{r optimal-model}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = _____, input_shape = ncol(train_images)) %>%
  layer_dense(units = 10, activation = _____)

network %>% compile(
  optimizer = "rmsprop",
  loss = _____,
  metrics = c("accuracy")
)

# use optimal number of epochs
history <- network %>% 
  fit(train_images, train_labels, 
      batch_size = 128, epochs = _____, 
      validation_split = 0.2)
```


# Part 3: Evaluate results

Now that we've found a pretty good model, let's go ahead and evaluate our
results on the test data.

## Metrics

```{r metrics}
metrics <- network %>% evaluate(test_images, test_labels, verbose = FALSE)
metrics
```

## Predictions

You can predict the probability of each class. Note that column 1 is for
digit 0, column 2 is for digit 1, ..., column 10 is for digit 9.

```{r predict-probabilities}
network %>% predict_proba(test_images[1:10,])
```

Or you can predict the class

```{r predict-classes}
network %>% predict_classes(test_images[1:10,])
```

Lets get all our predictions and the actual responses for the test set.

```{r predictions-vs-actuals}
predictions <- network %>% predict_classes(test_images)
actual <- mnist$test$y
```

# Confusion Matrix

We can see how many missed predictions our model had.

```{r missed-predictions}
missed_predictions <- sum(predictions != actual)
missed_predictions
```

We can see which digits our model confuses the most by analyzing the confusion
matrix.

* 9s are often confused with 4s
* 8s are often confused with 2s & 3s
* etc.

```{r confusion-matrix}
caret::confusionMatrix(factor(predictions), factor(actual))
```

We can also visualize this with the following:

```{r visual-confusion-matrix}
data.frame(target = mnist$test$y,
                      prediction = network %>%
                        predict_classes(test_images)) %>% 
  filter(target != prediction) %>% 
  group_by(target, prediction) %>%
  count() %>%
  ungroup() %>%
  mutate(perc = n/nrow(.)*100) %>% 
  filter(n > 1) %>% 
  ggplot(aes(target, prediction, size = n)) +
  geom_point(shape = 15, col = "#9F92C6") +
  scale_x_continuous("Actual Target", breaks = 0:9) +
  scale_y_continuous("Prediction", breaks = 0:9) +
  scale_size_area(breaks = c(2,5,10,15), max_size = 5) +
  coord_fixed() +
  ggtitle(paste(missed_predictions, "mismatches")) +
  theme_classic() +
  theme(rect = element_blank(),
        axis.line = element_blank(),
        axis.text = element_text(colour = "black")) +
  labs(caption = 'Courtesy Rick Scavetta')

```


# Visualize missed predictions

Lastly, lets check out those mispredicted digits.

```{r mis-predicted-digits}
missed <- which(predictions != actual)
plot_dim <- ceiling(sqrt(length(missed)))

par(mfrow = c(plot_dim, plot_dim), mar = c(0,0,0,0))
for (i in missed) {
  plot(as.raster(mnist$test$x[i,,], max = 255))
}
```

If we look at the predicted vs actual we can reason about why our model 
mispredicted some of the digits.

```{r}
par(mfrow = c(4, 4), mar = c(0,0,2,0))

for (i in missed[1:16]) {
  plot(as.raster(mnist$test$x[i,,], max = 255)) 
  title(main = paste("Predicted:", predictions[i]))
}
```